env:
  _target_: 'envs.base_envs.flatworld.FlatWorld'
  render_mode: 'human'
  continuous_actions: True
classes:
  discrete: <class 'envs.flatworld.flatworld'>
  continuous: <class 'envs.flatworld.flatworld_continuous'>
logger:
  dir_name: flatworld_stl
ltl:
  autobuild: False
  formula: "G(F(y & X(F(r) & X(F(g))))) & G~b"  #"G(F(y & X(F(r)))) & G~b" 
  oa_type: ldba
  rabinizer: ./rabinizer4/bin/ltl2ldba
ppo:
  action_std: 0.69                  # starting std for action distribution (Multivariate Normal)
  temp_decay_type: 'None'
  temp_decay_rate: 0.99       # linearly decay action_std (action_std = action_std - action_std_decay_rate)
  min_action_temp: 0.7             # stop decay after action_std <= min_action_std and switch to learned std
  temp_decay_freq__n_episodes: 10       # action_std decay frequency (in num timesteps)
  K_epochs: 1               # update policy for K epochs in one PPO update
  batch_size: 128               # update policy for K epochs in one PPO update
  eps_clip: 0.4          # clip parameter for PPO
  lr_actor: 0.001       # learning rate for actor network
  lr_critic: 0.01       # learning rate for critic network
  update_timestep: 1 # T * 4
  update_freq__n_episodes: 1
  n_traj: 5000 
  n_pretrain_traj: 0
  var_denominator: 1
  alpha: 0.35
  T: 60
q_learning:
  batches_per_update: 5
  batch_size: 128
  update_freq__n_episodes: 1
  init_temp: .5
  temp_decay_freq__n_episodes: 100
  temp_decay_rate: .9
  temp_decay_type: 'exponential'
  min_action_temp: 0.05
  stl_lr: .00001
  reward_lr: .00001
  iterations_per_target_update: 500
  n_traj: 10000
  T: 60
testing:
  testing_freq__n_episodes: 10
  num_rollouts: 5
gamma: .98
n_grad_updates: 1
delta: .1
n_seeds: 1
init_seed: 99
replay_buffer_size: 10000
lambda: 400.0
lambda_qs: 400.0
visualize: True
load_path: ""  # 0 if not pre-loading a model, otherwise a string path to the model.
model_name: "flatworld_model"
run_name: "dummy"
num_eval_trajs: 50
baseline: "quant"